{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading and exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook will walk you through the basic classes intended for the end use of the module and the dataset.\n",
    "It is written for and tested with dataset version 1.0.1 but should be compatible with all 1.0 versions. Please refer to the repository's `README.md` for download instructions.\n",
    "On top of python3.10, you will need jupyter in order to run this notebook at home. Warning: pdfs are not rendered in github preview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Proof Bank and Samples\n",
    "Let us first make the necessary imports and load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and verifying aethel_1.0.0a5.pickle...\n",
      "Loaded æthel version 1.0.0a5 containing 68763 samples.\n"
     ]
    }
   ],
   "source": [
    "from aethel import ProofBank\n",
    "dataset = ProofBank.load_data('../data/aethel_1.0.0a5.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The just initialized `dataset` item is an instance of a `ProofBank`, i.e. a simple container of `Sample` objects.\n",
    "It provides some basic functionality, like a `version` field that specifies the dataset's version and a `__len__` function that returns its size.\n",
    "\n",
    "More importantly, it allows us to retrieve a single `Sample` using standard python indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = dataset[2310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`Samples` are identified by their names, which are unique, and consist of 2 parts, a *prefix* (ending in xml) that specifies the name of the source file in Lassy, and (optionally) a *suffix* that alerts us to the fact that the original parse graph has been disassembled into multiple ones during preprocessing (this can happen for a number of reasons, but is mostly due to incomplete or underspecified annotations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'dpc-svb-000432-nl-sen.p.47.s.3(1).xml'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can inspect the raw sentence of the sample via its `sentence` property..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voor de hoogte van het AOW-pensioen maakt het geen verschil of u gehuwd bent of samenwoont .\n"
     ]
    }
   ],
   "source": [
    "print(sample.sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " ...and the data subset (train/dev/test) it belongs to via its `subset` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "print(sample.subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lexical Phrases and Items\n",
    "\n",
    "The lexical content of each sample is provided pre-tokenized and chunked by Lassy's annotations.\n",
    "\n",
    "Lexical phrases are stored in the `lexical_phrases` field of a `Sample`. \n",
    "Each `LexicalPhrase` is a wrapper around a\n",
    "* non-empty tuple of `LexicalItems` (*access via `items`*),\n",
    "* for which a `Type` is supplied (*access via `type`*).\n",
    "\n",
    "The full string can be accessed via property `string`, and the len of `LexicalItems` contained via `__len__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexicalPhrase(string=het, type=VNW, len=1)\n"
     ]
    }
   ],
   "source": [
    "lp7 = sample.lexical_phrases[7]\n",
    "print(lp7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each `LexicalItem` within a `LexicalPhrase` corresponds to a single word, and comes packed with some rudimentary token-level features. This allows us to assign a single type to multi-word expressions (rather common in Lassy), while still maintaining their token-level annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexicalItem(word='het', pos='det', pt='vnw', lemma='het')\n"
     ]
    }
   ],
   "source": [
    "print(lp7.items[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Most lexical phrases participate in the proof-derivation as lexical constants, typed as specified. \n",
    "\n",
    "Some, however, don't (i.e. those assigned default dependencies, like punctuation symbols) -- which is why their provision *outside* the proof is necessary for sample representation not to be lossy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Proofs, Judgements and Terms\n",
    "\n",
    "The syntactic analysis of each sample resides in its `proof` field, and is a `Proof` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "proof = sample.proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A `Proof` is an inductive datatype that faithfully mirrors the Natural Deduction presentation of the underlying type theory, i.e. dependency-enhanced Lambek with permutations (or Modal Multiplicative Intuitionistic Linear Logic).\n",
    "\n",
    "It contains three named fields:\n",
    "* `premises` --  a (possibly empty) tuple of premise `Proofs`\n",
    "* `conclusion` -- a conclusion `Judgement`, and\n",
    "* `rule` -- a `Rule`. \n",
    "\n",
    "Where a `Judgement` consists of \n",
    "* a `Structure` of `Variables` (hypothetical elements) and/or `Constants` (lexical constants)\n",
    "\n",
    "For brevity, printing a `Proof` will only print its `conclusion` field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "〈c0, 〈〈c3, 〈〈c4〉det, c5〉obj1〉mod, 〈c1〉det, c2〉obj1〉mod, c6, 〈〈c8〉mod, c9〉obj1, 〈c10, 〈c14, 〈c15〉cnj, 〈c13, 〈c12〉predc〉cnj, 〈c11〉su〉cmpbody〉su, 〈c7〉sup ⊢ ▾mod(c0 ▵obj1(▾mod(c3 ▵obj1(▾det(c4) c5)) (▾det(c1) c2))) (c6 ▵obj1(▾mod(c8) c9) ▵su(c10 ▵cmpbody(c14 ▵cnj(c15) ▵cnj(c13 ▵predc(c12)) ▵su(c11))) ▵sup(c7)) : SMAIN\n"
     ]
    }
   ],
   "source": [
    "print(proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Shortcut properties `Proof.structure`, `Proof.type`, `Proof.term` provide access to fields and properties nested in `Proof.conclusion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMAIN\n"
     ]
    }
   ],
   "source": [
    "print(proof.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For a more holistic inspection of a proof, you can use the `LassyExtraction.utils.tex` submodule to cast samples and proofs to compilable tex code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from aethel.utils.tex import sample_to_tex\n",
    "tex_code = sample_to_tex(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The tex code can be saved to a file and compiled externally. If you have pdflatex installed, you should also be able to directly invoke the `compile_tex` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from aethel.utils.tex import compile_tex\n",
    "compile_tex(tex_code, 'tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The compiled end result can be found as `tmp.pdf` in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Searching the dataset\n",
    "`scripts/search.py` provides some simple first-order filtering tools."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from scripts.search import search, length_between, of_type, must_contain_rules, may_only_contain_rules, contains_word, Query, Sample\n",
    "from aethel.mill.proofs import Logical, Atom"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `search` function takes a (subset of the) dataset, a logical Query plus (optionally) a maximum number of hits, and returns a list of matching samples.\n",
    "The below expression filters the first 50 items that contain exclusively applicative terms:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "is_simple_applicative = may_only_contain_rules({Logical.Constant, Logical.ArrowElimination, Logical.BoxElimination, Logical.DiamondIntroduction})\n",
    "simple_applicative = list(search(bank=dataset, query=is_simple_applicative, num_hits=50))\n",
    "compile_tex(sample_to_tex(simple_applicative[33]), 'applicative')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Queries can be composed, combined and negated like standard logical expressions.\n",
    "This next one finds proofs that are 5 to 7 phrases long, contain at least one λ abstraction, but do not contain the word \"en\":"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "higher_order = list(search(bank=dataset, query=must_contain_rules({Logical.Variable}) & length_between(5, 7) & (~ contains_word('en')), num_hits=10))\n",
    "compile_tex(sample_to_tex(higher_order[0]), 'higher_order')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Custom queries are also easy to write. The below query filters sentences that end with a question mark and are typed as a WH-question."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def ends_with_qmark() -> Query:\n",
    "    def f(s: Sample) -> bool: return s.sentence.endswith('?')\n",
    "    return Query(f)\n",
    "\n",
    "questions = list(search(bank=dataset, query=ends_with_qmark() & of_type(Atom('WHQ')), num_hits=10))\n",
    "compile_tex(sample_to_tex(questions[4]), 'question')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}